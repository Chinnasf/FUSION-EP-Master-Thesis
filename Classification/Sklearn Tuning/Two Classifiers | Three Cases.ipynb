{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60003dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248f0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import tokamakTK\n",
    "\n",
    "from tokamakTK import get_metrics_for_decreasing\n",
    "\n",
    "import pydotplus\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../../data/\"\n",
    "fig_path = \"../../../../LATEX/Latex Images/\"\n",
    "\n",
    "sstyle = 'seaborn-v0_8-poster'\n",
    "plt.style.use(sstyle)\n",
    "plt.rc('font',family = 'serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3fed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.45% of the data decreased alpha_R\n",
      "76.55% of the data did not decrease alpha_R\n"
     ]
    }
   ],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# REMOVING SPHERICAL TOKAMAKS\n",
    "#DB5 = DB5[~DB5.TOK.isin(['MAST', 'NSTX', 'START'])]\n",
    "\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d4fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_characteristics = [\"Q95\",\"BEIMHD\",\"PREMAG\",\n",
    "                          \"CONFIG\",\"DWDIA\",\"WMHD\",\"TORQ\",\"KAREA\", \"EPS\",\"MEFF\",\"VOL\",\"LCOULOMB\",\n",
    "                          \"IP\",\"RHOSTAR\",\"NUSTAR\",\"BETASTAR\"] \n",
    "TOK_characteristics = [\"TOK\",\"WALMAT\",\"DIVMAT\",\"LIMMAT\",\"AMIN\",\"BT\"]\n",
    "ELM = [\"ELMTYPE\",\"ELMFREQ\"]\n",
    "heating = [\"PECRH\", \"PICRH\", \"ICSCHEME\",\"AUXHEAT\",\"ECHMODE\",\"PELLET\"]\n",
    "impurities = [\"EVAP\",\"ZEFF\",\"ZEFFNEO\",\"PRAD\",\"POHM\",\"ENBI\",\"PNBI\"]\n",
    "power = [\"PLTH\",\"PFLOSS\"]\n",
    "temperatures = [\"TAV\",\"TEV\",\"TIV\"]\n",
    "fast_particles = [\"NESOL\",\"WFFORM\",\"WFICFORM\",\"OMEGACYCL\",\"NEL\"] \n",
    "\n",
    "features = plasma_characteristics + TOK_characteristics + ELM + \\\n",
    "           heating + impurities + power + temperatures + fast_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f42245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# 3 subsets to study\n",
    "research_features = ['NEL','TAV','BT','RHOSTAR','NUSTAR','BETASTAR'] + categorical_features\n",
    "entropy_low_MCL = ['WFFORM', 'RHOSTAR', 'DWDIA', 'BETASTAR', 'POHM', 'NEL', 'NUSTAR', 'PLTH'] + categorical_features\n",
    "low_MLC = ['BEIMHD', 'BETASTAR', 'BT', 'DWDIA', 'ELMFREQ', 'IP', 'NEL', 'NESOL', 'NUSTAR', \n",
    "           'PECRH', 'PFLOSS', 'PICRH', 'POHM', 'PRAD', 'Q95', 'RHOSTAR', 'TIV', 'TORQ', \n",
    "           'WFFORM', 'WFICFORM', 'ZEFF', 'ZEFFNEO']\n",
    "FEATURES = [research_features, entropy_low_MCL, low_MLC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clean_data(features, db5):\n",
    "    DB5 = tokamakTK.clean_categorical_data(db5)\n",
    "    # Needed to respectively clean each dtype\n",
    "    num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "    cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "    data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "    data_cat = DB5[cat_features]\n",
    "    data_num = tokamakTK.clean_numerical_data(data_num)\n",
    "    data_ = pd.concat([data_num,\n",
    "                      (pd.concat([\n",
    "                           DB5[[\"label\"]], \n",
    "                           tokamakTK.encode_categorical_ohe(data_cat)\n",
    "                          ], axis=1)\n",
    "                      )],\n",
    "                      axis=1)\n",
    "    X = data_.drop(\"label\", axis=1)\n",
    "    y = data_[\"label\"]\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=71, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def define_model(X_train, y_train, model_=\"RF\"):\n",
    "    gp = GaussianProcessClassifier(random_state=71, n_jobs=-1)\n",
    "    if model_ == \"RF\":\n",
    "        model = RandomForestClassifier(min_samples_split=2,  \n",
    "                                min_samples_leaf=1,\n",
    "                                min_impurity_decrease=0.0007,\n",
    "                                n_estimators = 90,\n",
    "                                max_depth = 23,\n",
    "                                criterion='entropy',\n",
    "                                max_features=None,\n",
    "                                n_jobs=-1,\n",
    "                                random_state=71\n",
    "                               )\n",
    "    else:\n",
    "        model = GaussianProcessClassifier(random_state=71, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def get_predictions(model, X_test):\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def get_PRCs(results, label_pos=[\"lower left\"]*3, saveFig=False, file_name=\"\"):\n",
    "    LABELS = [\"Research\", \"Entropy with low MCL\", \"Low MCL\"]\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5.7))\n",
    "    for i, result in enumerate(results):\n",
    "        axs[i].plot(result[1], result[0], marker='.', label=LABELS[i], c=\"#A84547\")\n",
    "        axs[i].grid(alpha=0.5)\n",
    "        axs[i].legend(loc=label_pos[i])\n",
    "    axs[0].set_ylabel(\"Precision\\n\", fontsie=21)\n",
    "    axs[1].set_xlabel(\"\\nRecall\", fontsie=21)\n",
    "    if saveFig:\n",
    "        plt.savefig(fig_path+file_name+\".pdf\", format=\"pdf\", dpi=800, bbox_inches='tight');\n",
    "    plt.show()\n",
    "    \n",
    "def get_importance(model, X_training, X_test, y_test, n_permutations=200, metric=\"f1_macro\"):\n",
    "    \"\"\"\n",
    "    Returns pd.DataFrame with Ordered Feature Importance based on Permutations\n",
    "    \"\"\"\n",
    "    Importance = pd.DataFrame(np.zeros((len(X_training.columns), 2)), \n",
    "                              index=X_training.columns, columns=[\"mean\", \"std\"])\n",
    "    permutation = permutation_importance(model, X_test, y_test, n_repeats=n_permutations, \n",
    "                                         n_jobs=-1,\n",
    "                                         scoring=metric,\n",
    "                                         random_state=71)\n",
    "    for i in permutation.importances_mean.argsort()[::-1]:\n",
    "        if permutation.importances_mean[i] - 2 * permutation.importances_std[i] > 0:\n",
    "            Importance.loc[X_training.columns[i], \"mean\"] = np.round(permutation.importances_mean[i],4)\n",
    "            Importance.loc[X_training.columns[i], \"std\"] = np.round(permutation.importances_std[i],4)\n",
    "    Importance.sort_values(\"mean\", ascending=False, inplace=True)\n",
    "    return Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c113a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA = [split_clean_data(f, db5=DB5) for f in FEATURES]\n",
    "GaussianP  = [define_model(split_data[0], split_data[2], model_=\"GP\") for split_data in SPLIT_DATA]\n",
    "RdForests  = [define_model(split_data[0], split_data[2], model_=\"RF\") for split_data in SPLIT_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bddc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianP_preds = [get_predictions(GaussianP[i], SPLIT_DATA[i][1]) for i in range(3)]\n",
    "RdForests_preds = [get_predictions(RdForests[i], SPLIT_DATA[i][1]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3f15198",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_PRCs = [precision_recall_curve(SPLIT_DATA[i][3], GaussianP_preds[i][:,1]) for i in range(3)]\n",
    "RF_PRCs = [precision_recall_curve(SPLIT_DATA[i][3], RdForests_preds[i][:,1]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_PRCs(GP_PRCs, saveFig=True, file_name=\"GP_PRCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f31a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_PRCs(RF_PRCs, saveFig=True, file_name=\"RF_PRCs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "140c83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = pd.DataFrame(np.zeros((6,4)), columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1score\"],\n",
    "            index=[f\"GP{i}\" for i in range(3)]+[f\"RF{i}\" for i in range(3)])\n",
    "\n",
    "for i in range(3):\n",
    "    gp_metrics = get_metrics_for_decreasing(GaussianP[i], X_test=SPLIT_DATA[i][1], y_test=SPLIT_DATA[i][3])\n",
    "    rf_metrics = get_metrics_for_decreasing(RdForests[i], X_test=SPLIT_DATA[i][1], y_test=SPLIT_DATA[i][3])\n",
    "    \n",
    "    METRICS.loc[f\"GP{i}\", \"Accuracy\"] = gp_metrics[0]\n",
    "    METRICS.loc[f\"GP{i}\", \"Precision\"] = gp_metrics[1]\n",
    "    METRICS.loc[f\"GP{i}\", \"Recall\"] = gp_metrics[2]\n",
    "    METRICS.loc[f\"GP{i}\", \"F1score\"] = gp_metrics[3]\n",
    "    \n",
    "    METRICS.loc[f\"RF{i}\", \"Accuracy\"] = rf_metrics[0]\n",
    "    METRICS.loc[f\"RF{i}\", \"Precision\"] = rf_metrics[1]\n",
    "    METRICS.loc[f\"RF{i}\", \"Recall\"] = rf_metrics[2]\n",
    "    METRICS.loc[f\"RF{i}\", \"F1score\"] = rf_metrics[3]\n",
    "    \n",
    "METRICS.index = [\"GP_research\", \"GP_entropyLM\", \"GP_lowMCL\", \"RF_research\", \"RF_entropyLM\", \"RF_lowMCL\"]\n",
    "METRICS.sort_values(\"F1score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f9a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_Imporance = [get_importance(GaussianP[i], \n",
    "                               SPLIT_DATA[i][0], SPLIT_DATA[i][1], SPLIT_DATA[i][3]) for i in range(3)]\n",
    "RF_Imporance = [get_importance(RdForests[i], \n",
    "                               SPLIT_DATA[i][0], SPLIT_DATA[i][1], SPLIT_DATA[i][3]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0786811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chime\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b43737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
