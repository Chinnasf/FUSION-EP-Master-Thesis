{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e24333d",
   "metadata": {},
   "source": [
    "**IMPORTANT**: what is shown here was developed after my thesis defence. This is just a different approach to reinforce my understanding in the topic.\n",
    "\n",
    "### Note to self: to make `cupy` work: `cthe`, `cnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import tokamakTK\n",
    "from tokamakTK import MyCounter, HUEOrder\n",
    "\n",
    "import pydotplus\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.subplots as plsp\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rc('font',family = 'serif')\n",
    "\n",
    "path = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# REMOVING SPHERICAL TOKAMAKS\n",
    "#DB5 = DB5[~DB5.TOK.isin(['MAST', 'NSTX', 'START'])]\n",
    "\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low entropy and low multicollinearity features\n",
    "features = ['WFICFORM', 'WFFORM', 'RHOSTAR', 'ZEFFNEO', 'DWDIA', 'BETASTAR', 'NUSTAR', 'PFLOSS', 'Q95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB5 = tokamakTK.clean_categorical_data(DB5)\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "data_cat = DB5[cat_features]\n",
    "\n",
    "data_num = tokamakTK.clean_numerical_data(data_num, SS_scaling=False, UL_scale=False)\n",
    "\n",
    "data_ = pd.concat([data_num,\n",
    "                  (pd.concat([\n",
    "                       DB5[[\"label\"]], \n",
    "                       tokamakTK.encode_categorical_ohe(data_cat)\n",
    "                      ], axis=1)\n",
    "                  )],\n",
    "                  axis=1\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_.drop(\"label\", axis=1)\n",
    "y = data_[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc335d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data, now having eval\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=71, stratify=y\n",
    "                                                   )\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.5, \n",
    "                                                    random_state=71, stratify=y_train\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages per dataset\n",
    "X_train.shape[0]/X.shape[0], X_eval.shape[0]/X.shape[0], X_test.shape[0]/X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6833941",
   "metadata": {},
   "source": [
    "# Grid Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for Random Forest\n",
    "parameters = {\n",
    "    'n_estimators': [400, 425, 475], #[int(i) for i in np.linspace(200, 500, 5)],\n",
    "    'max_depth': [18, 19, 20], #[int(i) for i in np.linspace(15,25, 5)],\n",
    "#    'min_impurity_decrease': [0.0004, 0.0005, 0.0006,0.0007],\n",
    "#    'min_samples_split':[2,3,4,5],\n",
    "#    'max_features':[None,len(features),\"sqrt\",\"log2\"],\n",
    "#    'criterion':[\"gini\", \"entropy\", \"log_loss\"],\n",
    "#    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(min_samples_split=2,\n",
    "                            #min_samples_leaf=1,\n",
    "                            random_state=71,\n",
    "                            n_jobs=-1,\n",
    "                            criterion=\"gini\",\n",
    "                            bootstrap=False,\n",
    "                            max_features=\"sqrt\"\n",
    "                           )\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, \n",
    "    parameters, \n",
    "    cv= StratifiedKFold(n_splits=10, shuffle=True, random_state=71),\n",
    "    scoring='recall',\n",
    "    #n_jobs=-1,\n",
    "    refit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d09d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ea04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score and params\n",
    "grid_search.best_params_, grid_search.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b679d",
   "metadata": {},
   "source": [
    "Results of first search grid\n",
    "\n",
    "```python\n",
    ">>> grid_search.best_params_, grid_search.best_score_ \n",
    "({'bootstrap': False,\n",
    "  'criterion': 'gini',\n",
    "  'max_depth': 20,\n",
    "  'max_features': 'sqrt',\n",
    "  'min_samples_split': 2,\n",
    "  'n_estimators': 220},\n",
    " 0.7325248392752777)\n",
    "```\n",
    "\n",
    "Second grid\n",
    "\n",
    "```Python\n",
    ">>> grid_search.best_params_, grid_search.best_score_ \n",
    "({'max_depth': 19, 'max_features': 'sqrt', 'n_estimators': 425},\n",
    " 0.7324956165984804)\n",
    "```\n",
    "\n",
    "\n",
    "Last grid\n",
    "\n",
    "```Python\n",
    ">>> grid_search.best_params_, grid_search.best_score_ \n",
    "({'max_depth': 19, 'n_estimators': 425}, 0.7324956165984804)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba614017",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c33c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(min_samples_split=2,  \n",
    "                            min_samples_leaf=1,\n",
    "                            n_estimators = 425,\n",
    "                            max_depth = 19,\n",
    "                            criterion='gini',\n",
    "                            max_features=\"sqrt\",\n",
    "                            random_state=71,\n",
    "                            n_jobs=2,\n",
    "                           )\n",
    "\n",
    "rf.fit(pd.concat([X_eval,X_train]).reset_index(),\n",
    "       pd.concat([y_eval,y_train]).reset_index()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7593bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
