{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f691e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ebacf",
   "metadata": {},
   "source": [
    "# RESEARCH VARIABLES\n",
    "## `cuML` Implementation - GPU Machine Learning Algorithms\n",
    "\n",
    "[Source](https://github.com/rapidsai/cuml)\n",
    "\n",
    "\"cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n",
    "\n",
    "cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn.\"\n",
    "\n",
    "\n",
    "Why do I want to implement this? because, the normal version takes above two hourse to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f834bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import tokamakTK\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cuml.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from cuml.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "from IPython.core.debugger import Pdb #Pdb().set_trace()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09db631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc6e00",
   "metadata": {},
   "source": [
    "Class distribution can be considered is skewed.\n",
    "\n",
    "## Setting Features of Interest and Treatment to Categorical and Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TAUTH','NEL','TAV','BT','RHOSTAR','NUSTAR','BETASTAR']\n",
    "\n",
    "\n",
    "DB5 = tokamakTK.clean_categorical_data(DB5)\n",
    "\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "data_cat = DB5[cat_features]\n",
    "\n",
    "data_num = tokamakTK.clean_numerical_data(data_num)\n",
    "\n",
    "data_ = pd.concat([data_num,\n",
    "                  (pd.concat([\n",
    "                       DB5[[\"label\"]], \n",
    "                       tokamakTK.encode_categorical_ohe(data_cat)\n",
    "                      ], axis=1)\n",
    "                  )],\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b787f68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = cudf.DataFrame(data_.drop(\"label\", axis=\"columns\"))\n",
    "y = cudf.Series( data_[\"label\"] )\n",
    "\n",
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=71, stratify=y)\n",
    "\n",
    "# Convert cuDF dataframes to cuPy arrays\n",
    "X_train_cupy = X_train.astype('float32').to_cupy().get()\n",
    "y_train_cupy = y_train.astype('float32').to_cupy().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211e9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [95, 100, 110],\n",
    "    'max_depth': [15, 20, 23],\n",
    "    'min_samples_split': [1, 2, 5, 7],\n",
    "    'min_samples_leaf': [1, 4, 5, 7],\n",
    "#    'max_features': ['sqrt', 'log2', None], # \n",
    "#    'criterion': ['gini', 'entropy'] not supported in cuML\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "{'max_depth': 23,\n",
    " 'max_features': 'sqrt',\n",
    " 'min_samples_leaf': 3,\n",
    " 'min_samples_split': 3,\n",
    " 'n_estimators': 95}\n",
    " \n",
    "(array([0.82245431, 0.91621984]),\n",
    " array([0.71590909, 0.95261324]),\n",
    " array([0.7654921 , 0.93406218]),\n",
    " array([ 440, 1435]))\n",
    " \n",
    "{'max_depth': 20,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 5,\n",
    " 'n_estimators': 92}\n",
    " \n",
    "(array([0.83289817, 0.9189008 ]),\n",
    " array([0.725    , 0.9554007]),\n",
    " array([0.77521264, 0.93679535]),\n",
    " array([ 440, 1435]))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rf = RandomForestClassifier(max_features=None,\n",
    "                            random_state=71\n",
    "                           )\n",
    "grid_search = GridSearchCV(rf, \n",
    "                           param_grid, \n",
    "                           cv= 10, #StratifiedKFold(n_splits=10, shuffle=True, random_state=71), \n",
    "                           scoring='recall', \n",
    "                           n_jobs=1, \n",
    "                           refit=True\n",
    "                          )\n",
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_train_cupy, y_train_cupy)\n",
    "\n",
    "\n",
    "# Takes less than 10 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and best model from the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train_cupy, y_train_cupy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f793027",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "y_pred = y_pred[1]\n",
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_val[y_val==1]) / len(y_val)\n",
    "plt.plot(recall, precision, marker='.', label='Default Random Forest', color=\"r\")\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill', color=\"k\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(\"Research Variables\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = best_model.predict(X_val)\n",
    "precision_recall_fscore_support(y_val.to_numpy(), y_pred_.to_numpy(), labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd07be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
