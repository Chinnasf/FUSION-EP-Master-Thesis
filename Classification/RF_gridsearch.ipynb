{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f691e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb374ce",
   "metadata": {},
   "source": [
    "# For general tuning after cuML analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f834bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tokamakTK\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09db631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.45% of the data decreased alpha_R\n",
      "76.55% of the data did not decrease alpha_R\n"
     ]
    }
   ],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc6e00",
   "metadata": {},
   "source": [
    "Class distribution can be considered is skewed.\n",
    "\n",
    "## Treatment to Categorical and Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2d96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ZEFFNEO', 'EPS', 'WFICFORM', 'BETASTAR', 'RHOSTAR', 'DWDIA', 'KAREA', 'NEL', \n",
    "            'OMEGACYCL', 'ENBI', 'WMHD', 'AMIN', 'TAV', 'WFFORM', 'PICRH', 'PLTH', 'MEFF', \n",
    "            'QCYL5', 'NUSTAR', 'PNBI', 'VOL', 'LCOULOMB', 'LHTIME', 'PFLOSS', 'BT', 'BEIMHD', \n",
    "            'POHM', 'ZEFF', 'PECRH', 'ELMFREQ', 'PRAD', 'NESOL', 'TORQ', 'TOK', 'WALMAT', 'DIVNAME', \n",
    "            'LIMMAT', 'HYBRID', 'EVAP', 'ECHMODE', 'PREMAG', 'DIVMAT', 'PELLET', 'ELMTYPE', 'AUXHEAT', 'CONFIG']\n",
    "\n",
    "\n",
    "DB5 = tokamakTK.clean_categorical_data(DB5)\n",
    "\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "data_cat = DB5[cat_features]\n",
    "\n",
    "data_num = tokamakTK.clean_numerical_data(data_num)\n",
    "\n",
    "data_ = pd.concat([data_num,\n",
    "                  (pd.concat([\n",
    "                       DB5[[\"label\"]], \n",
    "                       tokamakTK.encode_categorical_ohe(data_cat)\n",
    "                      ], axis=1)\n",
    "                  )],\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab770964",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_.drop(\"label\", axis=1)\n",
    "y = data_[\"label\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=71, stratify=y)\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators':[300,400,500],\n",
    "    'max_depth':[20,25,27],\n",
    "    'min_samples_split': [1, 2, 3],\n",
    "    'min_samples_leaf': [1 ,2, 3],\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "rf = RandomForestClassifier(\n",
    "                            criterion='entropy',\n",
    "                            max_features=None,\n",
    "                            random_state=71\n",
    "                           )\n",
    "grid_search = GridSearchCV(rf, \n",
    "                           param_grid, \n",
    "                           cv= StratifiedKFold(n_splits=10, shuffle=True, random_state=71), \n",
    "                           scoring='recall', \n",
    "                           n_jobs=1, \n",
    "                           refit=True\n",
    "                          )\n",
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and best model from the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 13:24  --  13:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3697773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24566c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "y_pred = y_pred[:, 1]\n",
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_val[y_val==1]) / len(y_val)\n",
    "plt.plot(recall, precision, marker='.', label='Default Random Forest', color=\"r\")\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill', color=\"k\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(\"General Variables\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e22187",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = best_model.predict(X_val)\n",
    "precision_recall_fscore_support(y_val, y_pred_, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bbbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ada672",
   "metadata": {},
   "source": [
    "## [Feature Importance Based on Mean Decrease in Impurity](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "\n",
    "Feature importances are provided by the fitted attribute `feature_importances_` and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.\n",
    "\n",
    "**WARNING**:  Impurity-based feature importances can be misleading for high cardinality features (many unique values). See Permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "\n",
    "feature_importances = pd.DataFrame({\"feature\": X.columns, \"importance\": importances})\n",
    "feature_importances = feature_importances.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(feature_importances)\n",
    "df.feature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "plt.bar(df[df.importance > 0.5e-2].feature, df[df.importance > 0.5e-2].importance, color=\"gray\")\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59710410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite Important\n",
    "df[df.importance > 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b46ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not so important\n",
    "df[df.importance < 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.importance == 0.000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688f08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
