{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ca5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tokamakTK\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ebf44",
   "metadata": {},
   "source": [
    "# ALL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# REMOVING SPHERICAL TOKAMAKS\n",
    "DB5 = DB5[~DB5.TOK.isin(['MAST', 'NSTX', 'START'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593eee5",
   "metadata": {},
   "source": [
    "Class distribution can be considered almost severly skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d8502",
   "metadata": {},
   "source": [
    "### Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b242a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low multicollinearity among numerical features\n",
    "features = ['ZEFFNEO', 'WFICFORM', 'BETASTAR', 'DWDIA', 'KAREA', 'NEL','ENBI', 'PICRH', 'MEFF', \n",
    "            'QCYL5', 'NUSTAR', 'PNBI', 'VOL', 'LHTIME', 'PFLOSS', 'BT', 'POHM', 'ZEFF', 'PECRH', \n",
    "            'ELMFREQ', 'TOK', 'WALMAT', 'DIVNAME', 'LIMMAT', 'HYBRID', 'EVAP', 'ECHMODE', 'PREMAG', \n",
    "            'DIVMAT', 'PELLET', 'ELMTYPE', 'AUXHEAT', 'CONFIG']\n",
    "\n",
    "DB5 = tokamakTK.clean_categorical_data(DB5)\n",
    "\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "data_cat = DB5[cat_features]\n",
    "\n",
    "data_num = tokamakTK.clean_numerical_data(data_num)\n",
    "\n",
    "data_ = pd.concat([data_num,\n",
    "                  (pd.concat([\n",
    "                       DB5[[\"label\"]], \n",
    "                       tokamakTK.encode_categorical_ohe(data_cat)\n",
    "                      ], axis=1)\n",
    "                  )],\n",
    "                  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_.drop(\"label\", axis=1)\n",
    "y = data_[\"label\"]\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=71, stratify=y)\n",
    "\n",
    "gp = GaussianProcessClassifier()\n",
    "\n",
    "gp.fit(X_train, y_train)\n",
    "y_pred= gp.predict_proba(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(gp.score(X_train,y_train))\n",
    "print(gp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep probabilities for the positive outcome only\n",
    "y_pred = y_pred[:, 1]\n",
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='Default GPC', c=\"g\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(\"All Variables of Interest\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = gp.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_pred_, labels=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61ce2e",
   "metadata": {},
   "source": [
    "### FEATURE IMPORTANCE\n",
    "\n",
    "\n",
    "One way to get the feature importance for a Gaussian Process classifier is to use the **permutation feature importance** method.\n",
    "\n",
    "For each feature, randomly permute its values in the test data, and compute the recall of the classifier on the permuted data. Repeat this process multiple times to get a more accurate estimate of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 100\n",
    "feature_importance = np.zeros(X_train.shape[1])\n",
    "\n",
    "recall = precision_recall_fscore_support(y_test, y_pred_, labels=[1,0])[1][0]\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    recall_diffs = np.zeros(n_permutations)\n",
    "    for j in range(n_permutations):\n",
    "        X_test_permuted = X_test.copy()\n",
    "        X_test_permuted.iloc[:, i] = np.random.permutation(X_test.iloc[:, i])\n",
    "        y_pred_permuted = gp.predict(X_test_permuted)\n",
    "        recall_permuted = precision_recall_fscore_support(y_test, y_pred_permuted, labels=[1,0])[1][0]\n",
    "        recall_diffs[j] = recall_permuted - recall\n",
    "    feature_importance[i] = np.mean(recall_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424066e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(abs(feature_importance))[::-1]\n",
    "plt.bar(range(X_train.shape[1]), abs(feature_importance[sorted_idx]))\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_idx], rotation=90)\n",
    "plt.ylabel('Feature importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
