{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f691e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e2404",
   "metadata": {},
   "source": [
    "# `cuML` Implementation - GPU Machine Learning Algorithms\n",
    "\n",
    "[Source](https://github.com/rapidsai/cuml)\n",
    "\n",
    "\"cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n",
    "\n",
    "cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn.\"\n",
    "\n",
    "\n",
    "Why do I want to implement this? because, the normal version takes above two hourse to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f834bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cuml.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n",
    "\n",
    "from IPython.core.debugger import Pdb #Pdb().set_trace()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09db631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.69% of the data decreased alpha_R\n",
      "59.31% of the data did not decrease alpha_R\n"
     ]
    }
   ],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"id_vs_frequency_decreasing_ds.csv\")\n",
    "#min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc6e00",
   "metadata": {},
   "source": [
    "Class distribution can be considered is skewed.\n",
    "\n",
    "## Treatment to Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2d96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_features = ['NEL','TAV','BT','RHOSTAR','NUSTAR','BETASTAR']\n",
    "\n",
    "TOK_characteristics = [\"DIVNAME\",\"WALMAT\",\"DIVMAT\",\"LIMMAT\"]\n",
    "categorical = [\"PREMAG\",\"HYBRID\",\"CONFIG\",\"ELMTYPE\",\"ECHMODE\",\n",
    "               \"ICSCHEME\",\"AUXHEAT\",\"EVAP\",\"PELLET\"] + TOK_characteristics \n",
    "\n",
    "DB5[categorical] = DB5[categorical].fillna('UNKNOWN')\n",
    "DB5[\"DIVNAME\"]   = DB5[\"DIVNAME\"].str.replace(\"NONAME\",\"UNKNOWN\",regex=False)\n",
    "\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"CC\",\"C\",regex=False)\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"TI1\",\"TI12\",regex=False)\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"TI2\",\"TI12\",regex=False)\n",
    "\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(DIV-I)|(DV-IPRE)|(DV-IPOST)\",\n",
    "                                            \"DV-I\",regex=True)\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(DIV-II)|(DV-IIc)|(DV-II-C)|(DV-IIb)|(DV-IIc)|(DV-IId)|(DV-IId)\",\n",
    "                                            \"DV-II\",regex=True)\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(MARK0)|(MARKI)|(MARKIIA)|(MARKGB)|(MARKGBSR)|\"+\n",
    "                                            \"(MARKIA)|(MARKIAP)|(MARKSR)|(MARKA)|(MARKP)\",\n",
    "                                            \"MARK\",regex=True)\n",
    "\n",
    "DB5[\"ICSCHEME\"]   = DB5[\"ICSCHEME\"].str.replace(\"OFF\",\"NONE\",regex=False)\n",
    "\n",
    "# Removing noise on heating scheme | the removed coluns are shots from 1996\n",
    "DB5 = DB5[~DB5[\"AUXHEAT\"].isin([\"UNKNOWN\"])]\n",
    "\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"CARBH\",\"C-H\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"CARB\",\"C\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"BOROC\",\"C-BO\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"(BOROA)|(BOROB)|(BOROX)|(BOR)\",\"BO\",regex=True)\n",
    "\n",
    "DB5[\"PELLET\"] = DB5[\"PELLET\"].str.replace(\"GP_D\",\"D\",regex=False)\n",
    "DB5[\"PELLET\"] = DB5[\"PELLET\"].str.replace(\"GP_H\",\"H\",regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d7aec",
   "metadata": {},
   "source": [
    "## Treatment to Numerical Data | `DB5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b5dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_characteristics = [\"QCYL5\",\"BEIMHD\",\"PREMAG\",\"LHTIME\",\"HYBRID\",\n",
    "                          \"CONFIG\",\"DWDIA\",\"WMHD\",\"TORQ\"\n",
    "                         ] \n",
    "TOK_characteristics = [\"TOK\",\"DIVNAME\",\"WALMAT\",\"DIVMAT\",\"LIMMAT\"]\n",
    "ELM = [\"ELMTYPE\",\"ELMFREQ\"]\n",
    "heating = [\"PECRH\", \"PICRH\", \"ICSCHEME\",\"AUXHEAT\"]\n",
    "impurities = [\"EVAP\",\"ZEFF\",\"ZEFFNEO\",\"PRAD\",\"POHM\",\"ENBI\"]\n",
    " # corrections on power loss | NBI Power lost by unconfined orbits\n",
    "power = [\"PLTH\",\"PFLOSS\"]\n",
    "temperatures = [\"TAV\",\"TEV\",\"TIV\"]\n",
    "# e-density in SOL | total due to NBI| total due to ICRH\n",
    "fast_particles = [\"NESOL\",\"WFFORM\",\"WFICFORM\"] \n",
    "research_features = ['NEL','TAV','BT','RHOSTAR','NUSTAR','BETASTAR']\n",
    "\n",
    "interesting_features = set(plasma_characteristics + TOK_characteristics + ELM + heating + \\\n",
    "                       impurities + power  + fast_particles)\n",
    "\n",
    "data_ = DB5.copy()\n",
    "data_ = data_[list(interesting_features)]\n",
    "\n",
    "num_features = data_.select_dtypes(include=['int', 'float']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3044ccd",
   "metadata": {},
   "source": [
    "This is what I had before to join comlumns.\n",
    "\n",
    "```Python\n",
    "DB5_ = pd.DataFrame(StandardScaler().fit_transform( DB5[num_features] ), columns = num_features)\n",
    "data = pd.concat([DB5_,DB5[categorical]], axis=1, join=\"inner\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6664e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DB5.copy()\n",
    "data = data[num_features + categorical + [\"label\",\"TOK\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573827db",
   "metadata": {},
   "source": [
    "### Treatment to Numerical Data and One Hot Encoding | `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "033329be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for tokamak in data[\"TOK\"].unique():\n",
    "    indx_tok = data[data['TOK'] == tokamak].index\n",
    "    data.loc[indx_tok, num_features] = data[num_features].apply(impute_with_median)\n",
    "\n",
    "data[num_features] = StandardScaler().fit_transform(data[num_features])\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "transformed = encoder.fit_transform(data[categorical])\n",
    "ohe_df = pd.DataFrame(transformed.toarray(),\n",
    "                     columns=encoder.get_feature_names_out(categorical)\n",
    "                     )\n",
    "data_ = pd.concat([data[num_features + [\"label\"]], ohe_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b787f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cudf.DataFrame(data_.drop(\"label\", axis=1))\n",
    "y = cudf.Series(data_[\"label\"])\n",
    "\n",
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=71, stratify=y)\n",
    "\n",
    "# Convert cuDF dataframes to cuPy arrays\n",
    "X_train_cupy = X_train.astype('float32').to_cupy().get()\n",
    "y_train_cupy = y_train.astype('float32').to_cupy().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(rf, \n",
    "                           param_grid, \n",
    "                           cv= 5, #StratifiedKFold(n_splits=10, shuffle=True, random_state=71), \n",
    "                           scoring='recall', \n",
    "                           verbose=1,\n",
    "                           n_jobs=1, \n",
    "                           refit=True\n",
    "                          )\n",
    "# Fit the GridSearchCV object to your training data\n",
    "grid_search.fit(X_train_cupy, y_train_cupy)\n",
    "\n",
    "\n",
    "\"\"\" chatGPT Answer ------------------------------------------------------------------------------\n",
    "In the context of scikit-learn's GridSearchCV function, the verbose parameter controls the \n",
    "amount of output that is printed during the grid search process. It allows you to specify \n",
    "the level of verbosity for logging information about the grid search progress.\n",
    "\n",
    "The verbose parameter takes an integer value, which determines the amount of output to be printed. \n",
    "Higher integer values result in more detailed output, while lower values result in less output. \n",
    "The default value is 0, which means no output.\n",
    "\n",
    "Here are the possible values for the verbose parameter in GridSearchCV:\n",
    "\n",
    "    0: No output (default)\n",
    "    1: Print a message for each completed fit\n",
    "    2: Print a message for each completed fit and for each 10% increment in progress\n",
    "    3: Print a message for each completed fit and for each 1% increment in progress\n",
    "        You can set the verbose parameter to an appropriate value based on your preference and the \n",
    "        amount of information you want to see during the grid search process. For example, \n",
    "        if you want to see detailed progress information, you can set verbose to 2 or 3. \n",
    "        If you prefer minimal output, you can set it to 0.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 20: 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and best model from the GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f793027",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "y_pred = y_pred[:, 1]\n",
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred)\n",
    "# plot the roc curve for the model\n",
    "no_skill = len(y_val[y_val==1]) / len(y_val)\n",
    "plt.plot(recall, precision, marker='.', label='Default Random Forest', color=\"r\")\n",
    "plt.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill', color=\"k\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(\"Research Variables\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = model.predict(X_val)\n",
    "precision_recall_fscore_support(y_val, y_pred_, labels=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ada672",
   "metadata": {},
   "source": [
    "## [Feature Importance Based on Mean Decrease in Impurity](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "\n",
    "Feature importances are provided by the fitted attribute `feature_importances_` and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.\n",
    "\n",
    "**WARNING**:  Impurity-based feature importances can be misleading for high cardinality features (many unique values). See Permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from trained RF classifier\n",
    "importances = model.named_steps[\"classifier\"].feature_importances_\n",
    "\n",
    "# Get feature names from column transformer\n",
    "cat_encoder = model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "feature_names = cat_encoder.get_feature_names_out(cat_features).tolist() + num_features\n",
    "\n",
    "# Create a pandas dataframe to display feature importances\n",
    "feature_importances = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "feature_importances = feature_importances.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(feature_importances)\n",
    "df.feature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "plt.bar(df[df.importance > 0.5e-2].feature, df[df.importance > 0.5e-2].importance, color=\"gray\")\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59710410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite Important\n",
    "df[df.importance > 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b46ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not so important\n",
    "df[df.importance < 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f69e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CONFIG\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7633d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
