{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a410de",
   "metadata": {},
   "source": [
    "Karina ChiÃ±as Fuenes | 18/04/2023 | Universiteit Gent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e291939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166576a",
   "metadata": {},
   "source": [
    "This notebook is based on the document:\n",
    "\n",
    "* _Feature Selection for Clustering_, [Manoranjan Dash and Huan Liu](https://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf), 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c10c77",
   "metadata": {},
   "source": [
    "# NO RESEARCH VARIABLES\n",
    "\n",
    "This notebook helps me understand if there is a structure among the variables of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tokamakTK\n",
    "from tokamakTK import get_regression\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "\n",
    "# Setting ELMy Dataset\n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_regression( DB5[DB5.label.isin([1])], DB2  )[1].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c6703",
   "metadata": {},
   "source": [
    "## Treatment to Data and Setting Features of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_characteristics = [\"BEIMHD\", \"BETASTAR\", \"CONFIG\", \"DWDIA\", \"EPS\", \"HYBRID\", \"IP\", \"KAREA\",\n",
    "                          \"LCOULOMB\", \"LHTIME\", \"MEFF\", \"NUSTAR\", \"PREMAG\", \"QCYL5\", \"RHOSTAR\",\n",
    "                          \"TORQ\", \"VOL\", \"WMHD\"\n",
    "                         ]\n",
    "TOK_characteristics = [\"AMIN\", \"BT\", \"DIVMAT\", \"DIVNAME\", \"LIMMAT\", \"TOK\", \"WALMAT\"]\n",
    "ELM = [\"ELMFREQ\", \"ELMTYPE\"]\n",
    "heating = [\"AUXHEAT\", \"ECHMODE\", \"ICSCHEME\", \"PECRH\", \"PELLET\", \"PICRH\"]\n",
    "impurities = [\"ENBI\", \"EVAP\", \"PNBI\", \"POHM\", \"PRAD\", \"ZEFF\", \"ZEFFNEO\"]\n",
    "power = [\"PFLOSS\", \"PLTH\"]\n",
    "temperatures = [\"TAV\", \"TEV\", \"TIV\"]\n",
    "fast_particles = [\"NEL\", \"NESOL\", \"OMEGACYCL\", \"WFFORM\", \"WFICFORM\"]\n",
    "\n",
    "features = (plasma_characteristics + TOK_characteristics + ELM + heating + \n",
    "                       impurities + power + temperatures  + fast_particles)\n",
    "\n",
    "data = DB5[DB5.label.isin([1])].copy()\n",
    "data = data[features+[\"DATE\"]].reset_index(drop=True)\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_features = cat_features[:-1] # Removing \"DATE\"\n",
    "\n",
    "data_num = data[num_features+[\"TOK\"]]\n",
    "data_cat = data[cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB5 = tokamakTK.clean_categorical_data(DB5)\n",
    "\n",
    "# Needed to respectively clean each dtype\n",
    "num_features = DB5[features].select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = DB5[features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "data_num = DB5[num_features+[\"TOK\",\"DATE\"]]\n",
    "data_cat = DB5[cat_features]\n",
    "\n",
    "data_num = tokamakTK.clean_numerical_data(data_num)\n",
    "data = pd.concat([data_cat, data_num], axis=1)\n",
    "\n",
    "data = data[sorted(data.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N,M = data.shape\n",
    "\n",
    "# used to get feature ranking\n",
    "feature_dic = dict( zip( list(range(M)), data.columns ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab483b",
   "metadata": {},
   "source": [
    "## ENTROPY IN DATASET | Categorical\n",
    "#### Using the concept of entropy to rank the importance of features.\n",
    "\n",
    "0. Computing the distance between two points\n",
    "\n",
    "Euclidean distance:\n",
    "\n",
    "$$\n",
    "    D^k_{ij} = \\left( \\frac{ x_{ik} - x_{jk} }{ max(F_k) - min(F_k) } \\right)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "    D_{ij} = \\left[ \\sum^M_{k=1} \\: D^k_{ij}\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "Where $F_k$ is the $k$-th column representing the $k$-th feature. With $k=1,\\dots,M$ The $x_{ik}$ is the $i$-th point of the $k$-th feature; the same for $j$. With $i$,$j=1\\ldots,N$. And $N$ being the total points of observations. Thus the similarity between two points\n",
    "\n",
    "Defined as:\n",
    "\n",
    "$$\n",
    "    S_{ij} = e^{-\\alpha \\cdot D_{ij}}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a parameter. Here $\\alpha = 0.5$. According to [Manoranjan Dash and Huan Liu](https://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf), If one plots similarity vs distance, the curve will have a biggercurvature for a larger $\\alpha$. \n",
    "\n",
    "1. Computing the similarity between two points with Humming distance:\n",
    "\n",
    "\\begin{equation}\n",
    "        S_{ij} = \\frac{1}{M}\\sum^M_{k=1} \\delta_{ij}(x^k); \\quad \\text{with} \\:\\: \\delta_{ij}(x^k) = \\begin{cases}\n",
    "    1,\\:\\text{if}\\:\\: x^k_i = x^k_j\\\\\n",
    "    0,\\:\\text{if}\\:\\: x^k_i \\neq x^k_j\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "With $M$ being the total number of features.\n",
    "\n",
    "\n",
    "2. The entropy of the dataset is:\n",
    "\n",
    "$$\n",
    "    E = -\\sum^N_{i=1}\\sum^N_{j=1} E_{ij}= -\\sum^N_{i=1}\\sum^N_{j=1}\\left[ S_{ij}\\cdot\\text{Log}\\left(S_{ij}\\right) +  \\left(1-S_{ij}\\right)\\cdot\\text{Log}\\left(1-S_{ij}\\right) \\right]\n",
    "$$\n",
    "\n",
    "In the expression above, $N$ corresponds to the total number of observations in dataset (rows). \n",
    "\n",
    "The ranking is done in an interative process: eliminate a column at a time and assess the disorder that the removal has caused by calculating the corresponding entropy. Let us sat that $E_{^-F_i} > E_{^-F_j}$ is the statement that the removal of feature $i$ of the dataset caused more disorder than the removal of feature $j$. One can then construct an array $\\vec{E_-} = \\{E_{F_k}\\}$, for $k=1,\\ldots,M$. Thus, the most important feature will be  max$(\\vec{E_-})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873807d",
   "metadata": {},
   "source": [
    "$\\delta_{ij}(x^k)\\:\\rightarrow$ `np.frompyfunc( lambda x,y: x-y, 2, 1).reduce( np.array( np.meshgrid(F_k,F_k) ) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f199c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_ranked_entropy = tokamakTK.get_ranked_features(data)\n",
    "ordered_features = list(features_ranked_entropy.index.map(feature_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(features_ranked_entropy.index, features_ranked_entropy.values, c=\"r\")\n",
    "plt.xlabel(\"Removed Feature\")\n",
    "plt.ylabel(\"Entropy of Dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ordered_features,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f388d2",
   "metadata": {},
   "source": [
    "### Comparing with the case not containing research variables\n",
    "\n",
    "```Python\n",
    "['ZEFFNEO', 'EPS', 'WFICFORM', 'BETASTAR', 'RHOSTAR', 'DWDIA', 'KAREA', 'NEL', 'OMEGACYCL', 'ENBI', 'WMHD', 'AMIN', 'TAV', 'WFFORM', 'PICRH', 'PLTH', 'MEFF', 'QCYL5', 'NUSTAR', 'PNBI', 'VOL', 'LCOULOMB', 'LHTIME', 'PFLOSS', 'BT', 'BEIMHD', 'POHM', 'ZEFF', 'PECRH', 'ELMFREQ', 'PRAD', 'NESOL', 'TORQ', 'TOK', 'WALMAT', 'LIMMAT', 'HYBRID', 'EVAP', 'ECHMODE', 'PREMAG', 'DIVMAT', 'PELLET', 'ELMTYPE', 'ICSCHEME', 'AUXHEAT', 'CONFIG']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ab644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
