{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a410de",
   "metadata": {},
   "source": [
    "Karina ChiÃ±as Fuenes | 18/04/2023 | Universiteit Gent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e291939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166576a",
   "metadata": {},
   "source": [
    "This notebook is based on the document:\n",
    "\n",
    "* _Feature Selection for Clustering_, [Manoranjan Dash and Huan Liu](https://www.public.asu.edu/~huanliu/papers/pakdd00clu.pdf), 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb9da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import FeatureSelectTK as fs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "path = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cd04cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.69% of the data decreased alpha_R\n",
      "59.31% of the data did not decrease alpha_R\n"
     ]
    }
   ],
   "source": [
    "# Obtained from Optimization\n",
    "\n",
    "min_subset_ids = pd.read_csv(path+\"id_vs_frequency_decreasing_ds.csv\")\n",
    "#min_subset_ids = pd.read_csv(path+\"R_ids_alpha_0.6357.csv\")\n",
    "\n",
    "DB2 = pd.read_csv(path+\"DB2P8.csv\")\n",
    "DB5 = pd.read_csv(path+\"SELDB5_SVD.csv\", low_memory=False) \n",
    "DB5 = DB5[DB5[\"PHASE\"].isin(['HGELM', 'HSELM', 'HGELMH', 'HSELMH'])]\n",
    "\n",
    "# There is two shots from DB2P8 missing in DB5\n",
    "missing_shots = DB2[~DB2.id.isin( DB5.id.values )].reset_index(drop=True)\n",
    "DB5 = pd.concat([DB5, missing_shots], axis=0, ignore_index=True)\n",
    "\n",
    "# Labeling shots that had great impact in decreasing alpha_R\n",
    "DB5.insert(loc=2,column=\"label\",value=[0]*len(DB5))\n",
    "DB5.loc[(DB5[DB5.id.isin(min_subset_ids.id)].index), \"label\"] = 1\n",
    "\n",
    "print(\n",
    "    f\"{ round( (len(min_subset_ids)/len(DB5))*100     ,2)  }% of the data decreased alpha_R\\n\" + \n",
    "    f\"{ round( (1 - len(min_subset_ids)/len(DB5))*100 ,2)  }% of the data did not decrease alpha_R\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c6703",
   "metadata": {},
   "source": [
    "## Treatment to Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8db1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOK_characteristics = [\"DIVNAME\",\"WALMAT\",\"DIVMAT\",\"LIMMAT\"]\n",
    "categorical = [\"PREMAG\",\"HYBRID\",\"CONFIG\",\"ELMTYPE\",\"ECHMODE\",\n",
    "               \"ICSCHEME\",\"AUXHEAT\",\"EVAP\",\"PELLET\"] + TOK_characteristics \n",
    "\n",
    "DB5[categorical] = DB5[categorical].fillna('UNKNOWN')\n",
    "DB5[\"DIVNAME\"]   = DB5[\"DIVNAME\"].str.replace(\"NONAME\",\"UNKNOWN\",regex=False)\n",
    "\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"CC\",\"C\",regex=False)\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"TI1\",\"TI12\",regex=False)\n",
    "DB5[\"DIVMAT\"] = DB5[\"DIVMAT\"].str.replace(\"TI2\",\"TI12\",regex=False)\n",
    "\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(DIV-I)|(DV-IPRE)|(DV-IPOST)\",\n",
    "                                            \"DV-I\",regex=True)\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(DIV-II)|(DV-IIc)|(DV-II-C)|(DV-IIb)|(DV-IIc)|(DV-IId)|(DV-IId)\",\n",
    "                                            \"DV-II\",regex=True)\n",
    "DB5[\"DIVNAME\"] = DB5[\"DIVNAME\"].str.replace(\"(MARK0)|(MARKI)|(MARKIIA)|(MARKGB)|(MARKGBSR)|\"+\n",
    "                                            \"(MARKIA)|(MARKIAP)|(MARKSR)|(MARKA)|(MARKP)\",\n",
    "                                            \"MARK\",regex=True)\n",
    "\n",
    "DB5[\"ICSCHEME\"]   = DB5[\"ICSCHEME\"].str.replace(\"OFF\",\"NONE\",regex=False)\n",
    "\n",
    "# Removing noise on heating scheme | the removed coluns are shots from 1996\n",
    "DB5 = DB5[~DB5[\"AUXHEAT\"].isin([\"UNKNOWN\"])]\n",
    "\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"CARBH\",\"C-H\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"CARB\",\"C\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"BOROC\",\"C-BO\",regex=True)\n",
    "DB5[\"EVAP\"] = DB5[\"EVAP\"].str.replace(\"(BOROA)|(BOROB)|(BOROX)|(BOR)\",\"BO\",regex=True)\n",
    "\n",
    "DB5[\"PELLET\"] = DB5[\"PELLET\"].str.replace(\"GP_D\",\"D\",regex=False)\n",
    "DB5[\"PELLET\"] = DB5[\"PELLET\"].str.replace(\"GP_H\",\"H\",regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc20709",
   "metadata": {},
   "source": [
    "## Treatment to Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c22b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_characteristics = [\"QCYL5\",\"BEIMHD\",\"PREMAG\",\"LHTIME\",\"HYBRID\",\n",
    "                          \"CONFIG\",\"DWDIA\",\"WMHD\",\"TORQ\"\n",
    "                         ] \n",
    "TOK_characteristics = [\"TOK\",\"DIVNAME\",\"WALMAT\",\"DIVMAT\",\"LIMMAT\"]\n",
    "ELM = [\"ELMTYPE\",\"ELMFREQ\"]\n",
    "heating = [\"PECRH\", \"PICRH\", \"ICSCHEME\",\"AUXHEAT\"]\n",
    "impurities = [\"EVAP\",\"ZEFF\",\"ZEFFNEO\",\"PRAD\",\"POHM\",\"ENBI\"]\n",
    " # corrections on power loss | NBI Power lost by unconfined orbits\n",
    "power = [\"PLTH\",\"PFLOSS\"]\n",
    "temperatures = [\"TAV\",\"TEV\",\"TIV\"]\n",
    "# e-density in SOL | total due to NBI| total due to ICRH\n",
    "fast_particles = [\"NESOL\",\"WFFORM\",\"WFICFORM\"] \n",
    "research_features = ['NEL','TAV','BT','RHOSTAR','NUSTAR','BETASTAR']\n",
    "\n",
    "interesting_features = set(plasma_characteristics + TOK_characteristics + ELM + heating + \\\n",
    "                       impurities + power  + fast_particles)\n",
    "\n",
    "data = DB5.copy()\n",
    "data = data[list(interesting_features)]\n",
    "\n",
    "num_features = data.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cat_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "def impute_with_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "for tokamak in data[\"TOK\"].unique():\n",
    "    indx_tok = data[data['TOK'] == tokamak].index\n",
    "    data.loc[indx_tok, num_features] = data[num_features].apply(impute_with_median)\n",
    "\n",
    "data[num_features] = StandardScaler().fit_transform(data[num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a29f809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N,M = data.shape\n",
    "\n",
    "# used to get feature ranking\n",
    "feature_dic = dict( zip( list(range(M)), data.columns ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab483b",
   "metadata": {},
   "source": [
    "## ENTROPY IN DATASET | Categorical\n",
    "#### Using the concept of entropy to rank the importance of features.\n",
    "\n",
    "0. Computing the distance between two points\n",
    "\n",
    "Euclidean distance:\n",
    "\n",
    "$$\n",
    "    D^k_{ij} = \\left( \\frac{ x_{ik} - x_{jk} }{ max(F_k) - min(F_k) } \\right)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "    D_{ij} = \\left[ \\sum^M_{k=1} \\: D^k_{ij}\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "Where $F_k$ is the $k$-th column representing the $k$-th feature. With $k=1,\\dots,M$ The $x_{ik}$ is the $i$-th point of the $k$-th feature; the same for $j$. With $i$,$j=1\\ldots,N$. And $N$ being the total points of observations.\n",
    "\n",
    "Computing the similarity between two points\n",
    "\n",
    "Defined as:\n",
    "\n",
    "$$\n",
    "    S_{ij} = e^{-\\alpha \\cdot D_{ij}}\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is a parameter. Here $\\alpha = 0.5$.\n",
    "\n",
    "1. Computing the similarity between two points with Humming distance:\n",
    "\n",
    "\\begin{equation}\n",
    "        S_{ij} = \\frac{1}{M}\\sum^M_{k=1} \\delta_{ij}(x^k); \\quad \\text{with} \\:\\: \\delta_{ij}(x^k) = \\begin{cases}\n",
    "    1,\\:\\text{if}\\:\\: x^k_i = x^k_j\\\\\n",
    "    0,\\:\\text{if}\\:\\: x^k_i \\neq x^k_j\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "With $M$ being the total number of features.\n",
    "\n",
    "\n",
    "2. The entropy of the dataset is:\n",
    "\n",
    "$$\n",
    "    E = -\\sum^N_{i=1}\\sum^N_{j=1} E_{ij}= -\\sum^N_{i=1}\\sum^N_{j=1}\\left[ S_{ij}\\cdot\\text{Log}\\left(S_{ij}\\right) +  \\left(1-S_{ij}\\right)\\cdot\\text{Log}\\left(1-S_{ij}\\right) \\right]\n",
    "$$\n",
    "\n",
    "In the expression above, $N$ corresponds to the total number of observations in dataset (rows). \n",
    "\n",
    "The ranking is done in an interative process: eliminate a column at a time and assess the disorder that the removal has caused by calculating the corresponding entropy. Let us sat that $E_{^-F_i} > E_{^-F_j}$ is the statement that the removal of feature $i$ of the dataset caused more disorder than the removal of feature $j$. One can then construct an array $\\vec{E_-} = \\{E_{F_k}\\}$, for $k=1,\\ldots,M$. Thus, the most important feature will be  max$(\\vec{E_-})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873807d",
   "metadata": {},
   "source": [
    "$\\delta_{ij}(x^k)\\:\\rightarrow$ `np.frompyfunc( lambda x,y: x-y, 2, 1).reduce( np.array( np.meshgrid(F_k,F_k) ) )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ranked_entropy = fs.get_ranked_features(data, ddtype=\"cat\")\n",
    "ordered_features = list(features_ranked_entropy.index.map(feature_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.scatter(features_ranked_entropy.index, features_ranked_entropy.values, c=\"r\")\n",
    "plt.xlabel(\"Removed Feature\")\n",
    "plt.ylabel(\"Entropy of Dataset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ordered_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd2395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
